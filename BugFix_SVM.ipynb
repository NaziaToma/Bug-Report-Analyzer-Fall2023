{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN65XQDt5kjJvTI3pSCPRZg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaziaToma/BugType-and-BugFix-Predictor/blob/main/BugFix_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRJh5uWmsJLY",
        "outputId": "3fc60d1b-c374-412a-f9cf-19513f2e0d23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Loading datasets\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/BugFix_TrainingSet.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/BugFix_Testset.csv')"
      ],
      "metadata": {
        "id": "nMa2BlG5shXh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading necessary NLTK datasets\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize nltk's lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Defining a function to encapsulate preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    # Lemmatize each word\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF7e4erg-URv",
        "outputId": "6677a284-48eb-4e8f-a0f5-d6775350edf7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling NaN values with empty strings\n",
        "train_df.fillna('', inplace=True)\n",
        "test_df.fillna('', inplace=True)"
      ],
      "metadata": {
        "id": "QsN0TPE1s7bK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Short Description, bug report, and comments into a single text feature for each dataset\n",
        "train_df['combined_text'] = train_df['Short Description'] + ' ' + train_df['Bug Report'] + ' ' + train_df[train_df.columns[4:]].apply(lambda x: ' '.join(x.values.tolist()), axis=1)\n",
        "test_df['combined_text'] = test_df['Short Description'] + ' ' + test_df['Bug Report'] + ' ' + test_df[test_df.columns[4:]].apply(lambda x: ' '.join(x.values.tolist()), axis=1)"
      ],
      "metadata": {
        "id": "EystGQfbs_Fo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocessing to each row of the 'combined_text' column\n",
        "train_df['processed_text'] = train_df['combined_text'].apply(preprocess_text)\n",
        "test_df['processed_text'] = test_df['combined_text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "OpK2BSEH_iQb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the target variable for each dataset\n",
        "y_train = train_df['Resolution']\n",
        "y_test = test_df['Resolution']"
      ],
      "metadata": {
        "id": "57ftNmJQtFmB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a pipeline with TF-IDF Vectorizer and SVM Classifier\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "    ('clf', SVC(kernel= 'linear', probability=True))\n",
        "])"
      ],
      "metadata": {
        "id": "C_0GHMnstJep"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model on the training set\n",
        "pipeline.fit(train_df['processed_text'], y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = pipeline.predict(test_df['processed_text'])\n",
        "\n",
        "# Printing the classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NmSgQlFtMt0",
        "outputId": "63392814-5777-4c64-d119-4b588305585d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       FIXED       0.70      0.83      0.76        53\n",
            "     INVALID       0.72      0.63      0.67        46\n",
            "     WONTFIX       0.80      0.73      0.77        45\n",
            "\n",
            "    accuracy                           0.74       144\n",
            "   macro avg       0.74      0.73      0.73       144\n",
            "weighted avg       0.74      0.74      0.73       144\n",
            "\n"
          ]
        }
      ]
    }
  ]
}